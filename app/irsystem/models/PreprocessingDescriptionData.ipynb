{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import re\n",
    "import string\n",
    "from operator import itemgetter\n",
    "from nltk.stem import PorterStemmer\n",
    "from __future__ import print_function\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('Data-Set-Final.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanhtml(raw_html):\n",
    "    clean = re.compile('<.*?>')\n",
    "    cleantext = re.sub(clean, '', raw_html)\n",
    "    return cleantext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    \"\"\"Returns a list of words that make up the text.    \n",
    "    Params: {text: String}\n",
    "    Returns: List\n",
    "    \"\"\"\n",
    "    return list(filter(str.strip, list(map(lambda x: x, re.findall(r'[a-zA-Z]*', text)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stem(text):\n",
    "    stemmer=PorterStemmer()\n",
    "    stems = [stemmer.stem(w) for w in tokenize(text)]\n",
    "    return \" \".join(stems)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    text = str(text)\n",
    "    text = cleanhtml(text)\n",
    "    text = text.lower()\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    text = text.strip()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(data):\n",
    "    for (index,value) in data['Summary'].items():\n",
    "        value = preprocess_text(value)\n",
    "        value = stem(value)\n",
    "        data.loc[index,'Summary'] = value\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_feats = 5000\n",
    "doc_by_vocab = np.empty([len(data), n_feats])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sophiezhao/venv/lib/python3.5/site-packages/sklearn/feature_extraction/text.py:1039: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  if hasattr(X, 'dtype') and np.issubdtype(X.dtype, np.float):\n"
     ]
    }
   ],
   "source": [
    "def build_vectorizer(max_features, stop_words, max_df=0.8, min_df=10, norm='l2'):\n",
    "    \"\"\"Returns a TfidfVectorizer object with the above preprocessing properties.\n",
    "    \n",
    "    Params: {max_features: Integer,\n",
    "             max_df: Float,\n",
    "             min_df: Float,\n",
    "             norm: String,\n",
    "             stop_words: String}\n",
    "    Returns: TfidfVectorizer\n",
    "    \"\"\"\n",
    "    \n",
    "    result = TfidfVectorizer(max_features = max_features, stop_words = stop_words, max_df = max_df, min_df = min_df, norm = norm)\n",
    "    return result\n",
    "\n",
    "data = preprocess(data)\n",
    "tfidf_vec = build_vectorizer(n_feats, \"english\")\n",
    "doc_by_vocab = tfidf_vec.fit_transform([value for _,value in data['Summary'].items()]).toarray()\n",
    "index_to_vocab = {i:v for i, v in enumerate(tfidf_vec.get_feature_names())}\n",
    "movie_index_to_name = data['Title'].to_dict()\n",
    "movie_name_to_index = {v: k for k, v in movie_index_to_name.items()}\n",
    "num_movies = len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sim(mov1, mov2, input_doc_mat, movie_name_to_index):\n",
    "    \"\"\"Returns a float giving the cosine similarity of \n",
    "       the two movie transcripts.\n",
    "    \n",
    "    Params: {mov1: String,\n",
    "             mov2: String,\n",
    "             input_doc_mat: Numpy Array,\n",
    "             movie_name_to_index: Dict}\n",
    "    Returns: Float (Cosine similarity of the two movie transcripts.)\n",
    "    \"\"\"\n",
    "    idx1 = movie_name_to_index[mov1]\n",
    "    idx2 = movie_name_to_index[mov2]\n",
    "    movie1 = input_doc_mat[idx1,]\n",
    "    movie2 = input_doc_mat[idx2,]\n",
    "    dot_product = np.dot(movie1, movie2)\n",
    "    return dot_product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sophiezhao/venv/lib/python3.5/site-packages/sklearn/feature_extraction/text.py:1039: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  if hasattr(X, 'dtype') and np.issubdtype(X.dtype, np.float):\n"
     ]
    }
   ],
   "source": [
    "def build_movie_sims_cos(n_mov, movie_index_to_name, input_doc_mat, movie_name_to_index, input_get_sim_method):\n",
    "    \"\"\"Returns a movie_sims matrix of size (num_movies,num_movies) where for (i,j):\n",
    "        [i,j] should be the cosine similarity between the movie with index i and the movie with index j\n",
    "    \n",
    "    Params: {n_mov: Integer,\n",
    "             movie_index_to_name: Dict,\n",
    "             input_doc_mat: Numpy Array,\n",
    "             movie_name_to_index: Dict,\n",
    "             input_get_sim_method: Function}\n",
    "    Returns: Numpy Array\n",
    "    \"\"\"\n",
    "    result = np.zeros((n_mov, n_mov))\n",
    "    for i in range(n_mov):\n",
    "        for j in range(n_mov):\n",
    "            if i == j:\n",
    "                result[i,j] = 0\n",
    "            else:\n",
    "                mov1 = movie_index_to_name[i]\n",
    "                mov2 = movie_index_to_name[j]\n",
    "                result[i,j] = input_get_sim_method(mov1, mov2, input_doc_mat, movie_name_to_index)\n",
    "    \n",
    "            \n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.06473643 0.         ... 0.08444302 0.12262928 0.        ]\n",
      " [0.06473643 0.         0.         ... 0.03823808 0.06216535 0.        ]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " ...\n",
      " [0.08444302 0.03823808 0.         ... 0.         0.10011804 0.        ]\n",
      " [0.12262928 0.06216535 0.         ... 0.10011804 0.         0.        ]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "movie_sims_cos = build_movie_sims_cos(num_movies, movie_index_to_name, doc_by_vocab, movie_name_to_index, get_sim)\n",
    "print(movie_sims_cos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def best_match(n_mov, movie_sims_cos, data, movie_index_to_name, movie_name_to_index, dramas_enjoyed, dramas_disliked, preferred_genres, preferred_network, num_results):\n",
    "    feature_list = ['Summary_Similarity', 'Genre_Similarity', 'Network_Similarity', 'Total']\n",
    "    result = pd.DataFrame(0, index=np.arange(n_mov), columns=feature_list)\n",
    "    genres = set()\n",
    "    preferred_genres = [preprocess_text(value) for value in preferred_genres]\n",
    "    genres.update(preferred_genres)\n",
    "    for drama in dramas_enjoyed:\n",
    "        if drama in movie_name_to_index.keys():\n",
    "            index = movie_name_to_index[drama]\n",
    "            sim = movie_sims_cos[index,:]\n",
    "            result['Summary_Similarity']+= pd.Series(sim)\n",
    "            \n",
    "    for drama in dramas_disliked:\n",
    "        if drama in movie_name_to_index.keys():\n",
    "            index = movie_name_to_index[drama]\n",
    "            sim = movie_sims_cos[index,:]\n",
    "            result['Summary_Similarity']-= pd.Series(sim)\n",
    "            \n",
    "    for index, value in data.iterrows():\n",
    "        gen = str(value['Genre'])\n",
    "        gen = preprocess_text(gen)\n",
    "        drama_genres = set()\n",
    "        drama_genres.update(gen.split())\n",
    "        result.loc[index,'Genre_Similarity'] = len(genres.intersection(drama_genres))/len(genres.union(drama_genres))\n",
    "        if preferred_network == data.iloc[index]['Network']:\n",
    "            result['Network_Similarity']+=1\n",
    "    result['Total'] = result.sum(axis = 1)\n",
    "    result = result.sort_values(by='Total', ascending=False)\n",
    "    result = result[:num_results]\n",
    "    indices =  result.index.tolist()\n",
    "    best_dramas = pd.Series([movie_index_to_name[index] for index in indices],index = result.index)\n",
    "    result.insert(loc=0, column='Drama_Title', value=best_dramas)\n",
    "    result.reset_index()\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            Drama_Title  Summary_Similarity  Genre_Similarity  \\\n",
      "1175                          New Heart            0.358763          1.000000   \n",
      "1033  Obstetrics and Gynecology Doctors            0.231075          1.000000   \n",
      "209                       Hospital Ship            0.181787          1.000000   \n",
      "97                        Risky Romance            0.128045          1.000000   \n",
      "345                             Doctors            0.115321          1.000000   \n",
      "308        Romantic Doctor, Teacher Kim            0.386111          0.666667   \n",
      "511                               Blood            0.370741          0.666667   \n",
      "791                         Golden Time            0.299970          0.666667   \n",
      "1442                       Summer Scent            0.461861          0.500000   \n",
      "957                                Sign            0.273635          0.666667   \n",
      "\n",
      "      Network_Similarity     Total  \n",
      "1175                   0  1.358763  \n",
      "1033                   0  1.231075  \n",
      "209                    0  1.181787  \n",
      "97                     0  1.128045  \n",
      "345                    0  1.115321  \n",
      "308                    0  1.052777  \n",
      "511                    0  1.037408  \n",
      "791                    0  0.966637  \n",
      "1442                   0  0.961861  \n",
      "957                    0  0.940302  \n"
     ]
    }
   ],
   "source": [
    "best = best_match(num_movies, movie_sims_cos, data, movie_index_to_name, movie_name_to_index, [\"Doctor Stranger\",\"Doctors\",\"Emergency Couple\"], [], [\"Romance\",\"Medical\"], [\"fun\"], 10)      \n",
    "print(best)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
